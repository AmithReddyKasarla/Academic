{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmithReddyKasarla/AmithReddy_INFO5502_-Spring2022/blob/main/lab_assignment_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AypcIiatK4TW"
      },
      "source": [
        "## The third Lab-assignment (02/10/2022, 50 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvJcs9AsK4Ta"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZscoKUBYK4Tb"
      },
      "source": [
        "Question 1 (10 points). Fomulate your domain problem: Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_u-i9SzK4Tb"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "I would like to discuss on e-commerce reviews, data analystics will hugely impact this domain problem.\n",
        "Here I am using flipkart website to retrieve data and predict if a particular phone is good or bad. For this analysis we need to collect reviews and their feedback from different users on varoius websites.\n",
        "Here the reviews are not in numerical format, whereas they are in text reviews, so we need to encode the text into numeric or score based format.\n",
        "We should also be careful about the data and it should not be biased, and should ensure the data collected is from authorised user to get good accuracy.\n",
        "Prior to model building, we need to perform Exploratory Data Analysis like droping duplicates, invalid and other noise from the data. And also ensuring that the data is in a valid data type to perform computation.\n",
        "We should be careful with data because, as data increses the time take and amount will also increase so we need to be accurate of the data that we are collecting and saving it.\n",
        "And the data collected should is valid for our analysis. Here we will perform sentiment analysis, so we need to depend on maximum valid data. As most of the unwanted data can be removed like any expressions, special charaters etc..,\n",
        "There are many tools and techniques for web scraping like OctoParse, Scrapy, Mozenda etc..,\n",
        "I will be importing data from flipkart website using pandas and selenium packages, Therefore all this data can further saved into csv, txt or any other file format.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLwzVodgK4Tc"
      },
      "source": [
        "Question 2 (10 points). Collect your data to answer the research problem: Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HguLJe8NK4Td",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25aa9671-d4a2-4100-e5c8-01e4e4f28b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of DataFrame: 1000\n",
            "      Customer Name                                    Review      Year Rating\n",
            "     RAJESH KOTHURU                          Perfect product! Dec, 2016      5\n",
            "        Sai  Harish                                 Wonderful Nov, 2018      5\n",
            "     Parthraj singh                                 Must buy! Nov, 2018      5\n",
            "         Axay patel                                   Awesome Jul, 2017      5\n",
            "      Nidhi Thakrar                                   Superb! May, 2017      5\n",
            "  Flipkart Customer                         Awesome phone.... Jun, 2017      5\n",
            "       Sagar shetty It's simply Awesome Review After 4 months Feb, 2015      5\n",
            "     Shreeya Thakur                                 Excellent Dec, 2016      5\n",
            "        Vasant Iyer                            Classy product Jan, 2017      5\n",
            "   Dinesh Vuppalapu                       My Search Ends Here Apr, 2015      5\n",
            "Subas Chandra Patel                       Nither Bad Nor Good Oct, 2017      4\n",
            "   Vignesh Karthick                               Great Apple May, 2015      5\n",
            "       Madhu Kannan                                  Moderate Dec, 2016      2\n",
            "     Sparsh Mahajan                      Excellent product !! Oct, 2014      5\n",
            "    A. K. Choudhury Everything I expected it to be, so far... May, 2015      4\n",
            "     Varun Bhardwaj                 Great delievery time..... Oct, 2014      5\n",
            "     Asheesh Saxena                           Excellent Phone Oct, 2014      5\n",
            "     Ashutosh Kumar                       Sleekest iPhone yet Dec, 2014      4\n",
            "  Flipkart Customer                     Met some expectations Oct, 2016      4\n",
            "Ripu Daman Jaiswal.            Trust me, you won't regret it. May, 2017      5\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "#importing BeautifulSoup,requests and pandas Libraries\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "#I am creating a list to store the review details\n",
        "listValues=[] \n",
        "\n",
        "#Defining a function for getting contents of a page\n",
        "def getContents(): \n",
        "  html_text=requests.get('https://www.flipkart.com/apple-iphone-6-gold-16-gb/product-reviews/itmey3ndwjre2dgf?pid=MOBEYGPZAHZQMCKZ&lid=LSTMOBEYGPZAHZQMCKZQ79VZF&aid=overall&certifiedBuyer=false&sortOrder=MOST_HELPFUL&page='+str(i)+'').text\n",
        " #Here I am parsing html using BeautifulSoup and lxml \n",
        "  soup=bs(html_text,'lxml')\n",
        "  reviews=soup.find_all('div',class_='col _2wzgFH K0kLPL')\n",
        "  \n",
        "  return reviews\n",
        "\n",
        "#I am using for statement to consider multiple web pages with 10 datasets each\n",
        "for i in range(1,105):  \n",
        "  reviews=getContents()\n",
        "  retry=0\n",
        "\n",
        "#Here I am using while statement because i want to be sure regarding the comments to capture. Sometimes the contents of the page are not captured through requests, so retrying 10 times until the contents are captured is recommended.\n",
        "  while(retry<=10):\n",
        "#Now I am Breaking the loop,because if the comments are captured before the retry limit    \n",
        "      if(len(reviews)==10): \n",
        "        break\n",
        "      else:\n",
        "          reviews=getContents()\n",
        "      retry+=1\n",
        "\n",
        "#now using for loop to check every review in all the reviews\n",
        "  for review in reviews:   \n",
        "#Here I am extracting rating from page by filtering with the row class    \n",
        "    rating=review.find('div',class_='row').div.text \n",
        "#Now extracting titleReview from page by filtering with the  '_2-N8zT' class     \n",
        "    titleReview=review.find('p',class_='_2-N8zT').text \n",
        "#Now extracting customerName from page by filtering with the '_2sc7ZR _2V5EHH' class     \n",
        "    customerName=review.find('p',class_='_2sc7ZR _2V5EHH').text.strip('[').strip(']')\n",
        "#Here extracting year from page by filtering with the '_2sc7ZR' class     \n",
        "    year=''.join([value.text for index,value in enumerate(review.find_all('p',class_='_2sc7ZR')) if index!=0])\n",
        "#Finally appending all the customerName,titleReview,year and rating to the list     \n",
        "    listValues.append([customerName,titleReview,year,rating]) \n",
        "\n",
        "#creatingNoe I am a data frame using list which has all the details\n",
        "df=pd.DataFrame(listValues,columns=['Customer Name','Review','Year','Rating']) \n",
        "\n",
        "#Using drop function to remove NULL values from the dataframe and inplace=True to make changes in present dataframe\n",
        "df.dropna(inplace=True) \n",
        "df.drop_duplicates(inplace=True) \n",
        "#Now I am droping all the rows that are more than 1000\n",
        "df.drop(df.index[1000:],inplace=True) \n",
        "\n",
        "#Here printing the length of the Dataframe\n",
        "print(\"Length of DataFrame:\",len(df))\n",
        "#Now I am only diplaying 20 records \n",
        "print(df.head(20).to_string(index=False))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu6CWOk8K4Td"
      },
      "source": [
        "Question 3 (10 points). Understand the data quality: Search a second hand dataset (any dataset) from kaggle or other websites. Describe the data quality problem of the dataset and explain your strtegy to clean the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGlfA1krK4Td"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "'''\n",
        "\n",
        "Please write you answer here:\n",
        "Here, I am taking a data set from kaggle called Flight Price Prediction. The main motto is to predic the flight price. Before building the model we need to check the data quality.\n",
        "For this we need to perform Exploratory Data Analysis. In this we can see a lot of data quality problem which need to be rectified before stating with the model.\n",
        "First we need to check the attributes and there types because we cannot use object type attributes and should convert them into int. And check the duplicates and Null values.\n",
        "And moreover converting date format, Date of Journey, Arrival_Time, Source and Destination into required format to perform the desired actions.\n",
        "Below is the strategy for this type of data that I followed:\n",
        "1. Firstly, I imported the required packages.\n",
        "2. Then imported the dataset into train_df.\n",
        "3. Then I checked the components in the data with the help of train_df.head(). Here I can see the columns and there values, by this I can identify the coulumns which are useful and other which I can drop.\n",
        "   Also, what can we do to certain columns without knowing their data types, For example: Date_of_Journey is in DD/MM/YYY so we need to split this into different columns.\n",
        "4. Then I used append function to add test data set to training data set to perform actions on complete data.\n",
        "5. Then by using info() we can get the backend information of complete data. Here I can see most of the columns are in objet data type, which need to changed.\n",
        "6. I took Date_of_Journey to split it into different using str.split('/'). Then store date, Month and Year separetly.\n",
        "7. Then change the data type to int using astype(int)\n",
        "8. Next as the Date_of_Journey is splitted and stored in different columns, we can drop the Date_of_Journey column using drop(Date_of_Journey, axis=1, inplace = True)\n",
        "9. Next I used same process for Arrival_Time to split it and store it in Arrival_Hour and Arrival_min.\n",
        "10. Then change Arrival_Hour and Arrival_min into int datatype. Then droping the Arrival_Time.\n",
        "11. Similarly I have done to Dep_Time.\n",
        "12. Then in Total_Stops column I used map function to code them as 0 stop, 1 stop, 2 stops...\n",
        "13. So then I droped Route column because we already have no.of stops.\n",
        "14. Then we have Duration coulmn, I have repeated same step as Arrival_Time to split the column.\n",
        "15. Then I can find and noise in the data which is showin Duration as 5m for 2 data records, so if I analyse the Source and Destination it from Mumbai to Hyderadad, Which is impossible to complete in 5min. Do we can remove these two rows from the data.\n",
        "16. Then converting the Dur_hour into int and multiplying it into 60 to convert it into min format.\n",
        "17. Then I added Dur_hour, Dur_min and stored it into Trip_Dur\n",
        "18. Next droped both Dur_hour and Dur_min\n",
        "19. Then for the column Airline we have set of unique airlines that are operating. So this cannot be directly used for model building because of its data type. To rectify this I used Label Encoding method.\n",
        "20. By this Label Encoding, the uniques Airlines will be coded with a value.\n",
        "Thus, I have cleaned complete data. After this EDA process, we can go ahead for model building.\n",
        "\n",
        "\n",
        "For your reference, here is code's github link - https://github.com/AmithReddyKasarla/AmithReddy_INFO5502_-Spring2022/blob/main/EDA%20and%20Feature%20Engineering%20on%20Flight%20Price%20Prediction.ipynb\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPdl8-NKK4Te"
      },
      "source": [
        "Question 4 (20 points). Data cleaning: There are two datasets TwADR-L (from Twitter) and AskAPatient (Link: https://zenodo.org/record/55013#.YgU2NN-ZO4T) for medical concept\n",
        "normalization. However, the two datasets have serious data quality problems. Please read the introduction of the datasets and clean the two datasets by following the steps in the statement.\n",
        "\n",
        "In the original dataset, the TwADR-L had 48,057 training, 1,256 validation and 1,427 test examples. The test set (all\n",
        "test samples from 10 folds combined) consists of 765 unique phrases and 273 unique classes (medical concepts). The AskAPatient dataset contained 156,652 training, 7,926 validation, and 8,662 test examples. The entire test set (all test samples\n",
        "from 10 folds combined) consists of 3,749 unique phrases and 1,035 unique classes (medical concepts). The authors\n",
        "randomly split each dataset into ten equal folds, ran 10-fold cross validation and reported the accuracy averaged across the\n",
        "ten folds. \n",
        "\n",
        "We found that, in the original data set, many phrase-label pairs appeared multiple times within the same training data file\n",
        "and also across the training and test data sets in the same fold. In the AskAPatient data set, on average 35.82% of the test data overlapped with training data in the same fold. In the Twitter (TwADR-L) dataset, on average 8.62% of the test set had an overlap with the training data in the same fold. Having a large overlap between the training and the test data can potentially\n",
        "introduce bias in the model and contribute to high accuracy. It is not unlikely that the high model performance reported in the original paper may be triggered by the the large overlap between the training and test sets.\n",
        "\n",
        "Therefore to remove the bias, we further cleaned and recreated the training, validation, and test sets such that each\n",
        "phrase-label pair appears only once in the entire dataset (either in training, validation or test set).\n",
        "\n",
        "(1) First, we combined all examples in training, validation and test data from the original data set and then removed all\n",
        "duplicate phrase-label pairs (examples that have the same phrase and label pair and appear more than once in training/validation/test datasets). Table II shows the statistics of the new dataset (after removing duplicates). The Twitter data set had 3,157 unique phrase-label pairs and 2,220 unique labels (medical concepts) while 173 phrases had multiple labels (i.e., they were assigned to more than one label). Many concepts had only one example, and the concept that had the most number\n",
        "of examples had 36 phrases. On average, each concept had 1.42 examples. The AskAPatient data set had 4,496 unique phrase-label pairs, 1,036 unique labels while 26 phrases had multiple labels. Table III shows examples of phrases that had multiple labels. For example, ‘mad’ can be mapped to ‘anger’ or ‘rage’ and ‘sore’ can be mapped to ‘pain’ or ‘myalgia’.\n",
        "\n",
        "(2) Second, we remove all concepts that had less than five examples. The statistics of the final data are shown in Table IV.\n",
        "\n",
        "(3) Third, we divide all examples without multiple labels into random 10 folds such that each unique phrase-label pair\n",
        "appears once in one of the 10 test sets. We add the pairs with multiple labels into the training data. This final 10-folds\n",
        "dataset is used in all our experiments.\n",
        "\n",
        "(The original paper can be download on canvas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ncgMsprWK4Tf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b86ab400-984c-44ee-b0e4-89f003c0c8ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/sample_data/datasets.zip\n",
            "   creating: datasets/\n",
            "  inflating: datasets/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/datasets/\n",
            "  inflating: __MACOSX/datasets/._.DS_Store  \n",
            "   creating: datasets/AskAPatient/\n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-0.test.txt  \n",
            "   creating: __MACOSX/datasets/AskAPatient/\n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-0.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-0.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-0.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-0.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-0.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-1.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-1.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-1.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-1.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-1.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-1.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-2.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-2.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-2.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-2.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-2.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-2.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-3.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-3.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-3.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-3.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-3.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-3.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-4.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-4.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-4.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-4.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-4.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-4.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-5.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-5.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-5.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-5.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-5.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-5.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-6.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-6.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-6.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-6.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-6.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-6.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-7.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-7.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-7.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-7.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-7.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-7.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-8.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-8.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-8.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-8.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-8.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-8.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-9.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-9.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-9.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-9.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-9.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-9.validation.txt  \n",
            "   creating: datasets/TwADR-L/\n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-0.test.txt  \n",
            "   creating: __MACOSX/datasets/TwADR-L/\n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-0.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-0.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-0.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-0.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-0.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-1.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-1.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-1.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-1.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-1.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-1.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-2.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-2.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-2.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-2.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-2.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-2.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-3.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-3.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-3.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-3.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-3.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-3.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-4.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-4.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-4.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-4.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-4.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-4.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-5.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-5.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-5.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-5.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-5.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-5.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-6.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-6.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-6.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-6.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-6.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-6.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-7.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-7.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-7.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-7.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-7.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-7.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-8.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-8.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-8.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-8.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-8.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-8.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-9.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-9.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-9.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-9.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-9.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-9.validation.txt  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3c4c65b3-31ad-4eb0-b0ff-26d2246f4534\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TwADR-L</th>\n",
              "      <th>AskAPatient</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Unique_phrases</th>\n",
              "      <td>2944</td>\n",
              "      <td>4470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unique_labels</th>\n",
              "      <td>2220</td>\n",
              "      <td>1038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unique_phrase_label_pairs</th>\n",
              "      <td>3157</td>\n",
              "      <td>4507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Phrases with multiple labels</th>\n",
              "      <td>173</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Min examples per label</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Max examples per label</th>\n",
              "      <td>36</td>\n",
              "      <td>141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Avg examples per label</th>\n",
              "      <td>1.42</td>\n",
              "      <td>4.34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c4c65b3-31ad-4eb0-b0ff-26d2246f4534')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c4c65b3-31ad-4eb0-b0ff-26d2246f4534 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c4c65b3-31ad-4eb0-b0ff-26d2246f4534');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                             TwADR-L AskAPatient\n",
              "Unique_phrases                  2944        4470\n",
              "Unique_labels                   2220        1038\n",
              "Unique_phrase_label_pairs       3157        4507\n",
              "Phrases with multiple labels     173          35\n",
              "Min examples per label             1           1\n",
              "Max examples per label            36         141\n",
              "Avg examples per label          1.42        4.34"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "#Here I am importing all the required libraries\n",
        "import glob\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import csv\n",
        "#Here i am unzipping the file\n",
        "!unzip /content/sample_data/datasets.zip\n",
        "#Creating a dataframe\n",
        "result_df =  pd.DataFrame()\n",
        "#Running a loop for dataset\n",
        "for dataset in ['TwADR-L', 'AskAPatient']:\n",
        "    result = {}\n",
        "    newdata = pd.DataFrame()\n",
        "    for filepath in glob.glob('/content/datasets/{}/*.txt'.format(dataset)):\n",
        "        data = pd.read_csv(filepath, sep = \"\\t\", header = None, encoding= 'unicode_escape')\n",
        "        newdata = newdata.append(data)\n",
        "        \n",
        "    newdata = newdata.reset_index(drop=True)\n",
        "    newdata['phrase_label'] = newdata[1] + \" \" + newdata[2]\n",
        "    newdata.columns = ['id', 'labels', 'phrases', 'phrase-label']\n",
        "    newdata = newdata.astype({\"id\": str})\n",
        "       \n",
        "    for columns in newdata.columns:\n",
        "        newdata[columns] = newdata[columns].str.lower() \n",
        "\n",
        "#Here I am droping all the duplicates\n",
        "    newdata = newdata.drop_duplicates('phrase-label')\n",
        "\n",
        "#Now I am storing all the results of the table in dictionary\n",
        "    result['Unique_phrases'] = str(len(newdata['phrases'].unique()))\n",
        "    result['Unique_labels'] = str(len(newdata['labels'].unique()))\n",
        "    result['Unique_phrase_label_pairs'] = str(newdata.shape[0])\n",
        "    newdata1 = pd.DataFrame(newdata['phrases'].value_counts())\n",
        "    result['Phrases with multiple labels'] = str(newdata1[newdata1['phrases'] > 1].shape[0])\n",
        "    result['Min examples per label'] = str(newdata['labels'].value_counts().values.min())\n",
        "    result['Max examples per label'] = str(newdata['labels'].value_counts().values.max())\n",
        "    result['Avg examples per label'] = round(newdata['labels'].value_counts().mean(),2)\n",
        "    result_newdata = result_newdata.append(result, ignore_index=True)\n",
        "\n",
        "\n",
        "result_newdata = result_newdata.T\n",
        "# result_df.iloc[0:6].astype(int)\n",
        "    \n",
        "result_newdata.columns = ['TwADR-L', 'AskAPatient']\n",
        "\n",
        "result_newdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "#Defining a dataframe method to read all the test,train and validation files from githublink\n",
        "def generateDataFrame(inputDataSet:str,inputFile:str)->pd.DataFrame:\n",
        "  df=pd.DataFrame(); \n",
        "\n",
        "#Here I am unzipping th the data file\n",
        "!unzip /content/sample_data/datasets.zip\n",
        "result_df = pd.DataFrame()\n",
        "#Here I am iterating through both datasets \n",
        "for dataset in ['TwADR-L', 'AskAPatient']:\n",
        "    df = pd.DataFrame()\n",
        "    result = {}\n",
        "    for filepath in glob.glob('/content/datasets/{}/*.txt'.format(dataset)):\n",
        "        data = pd.read_csv(filepath, sep = \"\\t\", header = None, encoding= 'unicode_escape')\n",
        "        df = df.append(data)\n",
        "#Now I am creating combined dataframe by combining Each dataset text files.\n",
        "    df = df.reset_index(drop=True)\n",
        "    df['phrase_label'] = df[1] + \" \" + df[2]\n",
        "    df.columns = ['id', 'labels', 'phrases', 'phrase-label']\n",
        "    df = df.astype({\"id\": str})\n",
        "#Here I am converting to lower case\n",
        "    for columns in df.columns:\n",
        "        df[columns] = df[columns].str.lower() \n",
        "\n",
        "#Now I am droping duplicates that are in phrase-label pairs... \n",
        "    df = df.drop_duplicates('phrase-label')\n",
        "\n",
        "    index_list = []\n",
        "    for i in range(df.shape[0]):\n",
        "        if df['labels'].value_counts()[df.iloc[i]['labels']] < 5:\n",
        "            index_list.append(i)\n",
        "\n",
        "#I am droping the labels that have less than count of 5.\n",
        "    df.drop(df.index[index_list], inplace=True)\n",
        "    result = {}\n",
        "\n",
        "    if dataset == 'TwADR-L':\n",
        "      Twt_df = df\n",
        "    else:\n",
        "      Ask_df = df\n",
        " \n",
        "    # storing results of the table in dictionary\n",
        "    result['Unique_phrases'] = str(len(df['phrases'].unique()))\n",
        "    result['Unique_labels'] = str(len(df['labels'].unique()))\n",
        "    result['Unique_phrase_label_pairs'] = str(df.shape[0])\n",
        "    df1 = pd.DataFrame(df['phrases'].value_counts())\n",
        "    result['Phrases with multiple labels'] = str(df1[df1['phrases'] > 1].shape[0])\n",
        "    result['Min examples per label'] = str(df['labels'].value_counts().values.min())\n",
        "    result['Max examples per label'] = str(df['labels'].value_counts().values.max())\n",
        "    result['Avg examples per label'] = round(df['labels'].value_counts().mean(), 2)\n",
        "    # appending dictinary to dataframe\n",
        "    result_df = result_df.append(result, ignore_index=True)\n",
        "\n",
        "result_df = result_df.T\n",
        "result_df.columns = ['TwADR-L', 'AskAPatient']\n",
        "\n",
        "\n",
        "result_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apt-cyrW3tPZ",
        "outputId": "1cb6c344-a764-410f-fb39-04d4ae87f8f3"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "DATA AFTER REMOVING CONCEPTS THAT HAVE LESS THAN FIVE EXAMPLES\n",
            "                                TwADR-L  AskAPatient\n",
            "              # unique phrases      543         2494\n",
            "               # unique labels       65          228\n",
            "   # unique phrase-label pairs      617         1427\n",
            "# phrases with multiple labels      173           26\n",
            "      Min # examples per label        5            5\n",
            "      Max # examples per label       36           78\n",
            "      Avg # examples per label        9           11\n",
            "\n",
            "\n",
            "DATA AFTER REMOVING CONCEPTS THAT HAVE LESS THAN FIVE EXAMPLES\n",
            "SOCIAL MEDIA PHRASE                                                MULTI-LABELS(Medical Concepts)\n",
            "            shaking                                                  shivering, trembling, tremor\n",
            "                mad                                                                   anger, rage\n",
            "    have no emotion                                          emotional disorder, indifferent mood\n",
            "        mood swings                                        bipolar disorder , disturbance in mood\n",
            "               sore                                                                 pain, myalgia\n",
            "high blood pressure  increased venous pressure, hypertension,findings of increased blood pressure\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "lab_assignment_03.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}