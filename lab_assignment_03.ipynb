{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmithReddyKasarla/AmithReddy_INFO5502_-Spring2022/blob/main/lab_assignment_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AypcIiatK4TW"
      },
      "source": [
        "## The third Lab-assignment (02/10/2022, 50 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvJcs9AsK4Ta"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZscoKUBYK4Tb"
      },
      "source": [
        "Question 1 (10 points). Fomulate your domain problem: Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_u-i9SzK4Tb"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "I would like to discuss on e-commerce reviews, data analystics will hugely impact this domain problem.\n",
        "Here I am using flipkart website to retrieve data and predict if a particular phone is good or bad. For this analysis we need to collect reviews and their feedback from different users on varoius websites.\n",
        "Here the reviews are not in numerical format, whereas they are in text reviews, so we need to encode the text into numeric or score based format.\n",
        "We should also be careful about the data and it should not be biased, and should ensure the data collected is from authorised user to get good accuracy.\n",
        "Prior to model building, we need to perform Exploratory Data Analysis like droping duplicates, invalid and other noise from the data. And also ensuring that the data is in a valid data type to perform computation.\n",
        "We should be careful with data because, as data increses the time take and amount will also increase so we need to be accurate of the data that we are collecting and saving it.\n",
        "And the data collected should is valid for our analysis. Here we will perform sentiment analysis, so we need to depend on maximum valid data. As most of the unwanted data can be removed like any expressions, special charaters etc..,\n",
        "There are many tools and techniques for web scraping like OctoParse, Scrapy, Mozenda etc..,\n",
        "I will be importing data from flipkart website using pandas and selenium packages, Therefore all this data can further saved into csv, txt or any other file format.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLwzVodgK4Tc"
      },
      "source": [
        "Question 2 (10 points). Collect your data to answer the research problem: Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HguLJe8NK4Td",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e926d21d-83bc-4317-d2d6-a9d2920cdeac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of DataFrame: 1000\n",
            "      Customer Name                                    Review      Year Rating\n",
            "     RAJESH KOTHURU                          Perfect product! Dec, 2016      5\n",
            "        Sai  Harish                                 Wonderful Nov, 2018      5\n",
            "     Parthraj singh                                 Must buy! Nov, 2018      5\n",
            "         Axay patel                                   Awesome Jul, 2017      5\n",
            "      Nidhi Thakrar                                   Superb! May, 2017      5\n",
            "  Flipkart Customer                         Awesome phone.... Jun, 2017      5\n",
            "       Sagar shetty It's simply Awesome Review After 4 months Feb, 2015      5\n",
            "     Shreeya Thakur                                 Excellent Dec, 2016      5\n",
            "        Vasant Iyer                            Classy product Jan, 2017      5\n",
            "   Dinesh Vuppalapu                       My Search Ends Here Apr, 2015      5\n",
            "Subas Chandra Patel                       Nither Bad Nor Good Oct, 2017      4\n",
            "   Vignesh Karthick                               Great Apple May, 2015      5\n",
            "       Madhu Kannan                                  Moderate Dec, 2016      2\n",
            "     Sparsh Mahajan                      Excellent product !! Oct, 2014      5\n",
            "    A. K. Choudhury Everything I expected it to be, so far... May, 2015      4\n",
            "     Varun Bhardwaj                 Great delievery time..... Oct, 2014      5\n",
            "     Asheesh Saxena                           Excellent Phone Oct, 2014      5\n",
            "     Ashutosh Kumar                       Sleekest iPhone yet Dec, 2014      4\n",
            "  Flipkart Customer                     Met some expectations Oct, 2016      4\n",
            "Ripu Daman Jaiswal.            Trust me, you won't regret it. May, 2017      5\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "#importing BeautifulSoup,requests and pandas Libraries \n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "#I am creating a list to store the review details\n",
        "listValues=[] \n",
        "\n",
        "#Defining a function for getting contents of a page\n",
        "def getContents(): \n",
        "  html_text=requests.get('https://www.flipkart.com/apple-iphone-6-gold-16-gb/product-reviews/itmey3ndwjre2dgf?pid=MOBEYGPZAHZQMCKZ&lid=LSTMOBEYGPZAHZQMCKZQ79VZF&aid=overall&certifiedBuyer=false&sortOrder=MOST_HELPFUL&page='+str(i)+'').text\n",
        " #Here I am parsing html using BeautifulSoup and lxml \n",
        "  soup=bs(html_text,'lxml')\n",
        "  reviews=soup.find_all('div',class_='col _2wzgFH K0kLPL')\n",
        "  \n",
        "  return reviews\n",
        "\n",
        "#I am using for statement to consider multiple web pages with 10 datasets each\n",
        "for i in range(1,105):  \n",
        "  reviews=getContents()\n",
        "  retry=0\n",
        "\n",
        "#Here I am using while statement because i want to be sure regarding the comments to capture. Sometimes the contents of the page are not captured through requests, so retrying 10 times until the contents are captured is recommended.\n",
        "  while(retry<=10):\n",
        "#Now I am Breaking the loop,because if the comments are captured before the retry limit    \n",
        "      if(len(reviews)==10): \n",
        "        break\n",
        "      else:\n",
        "          reviews=getContents()\n",
        "      retry+=1\n",
        "\n",
        "#now using for loop to check every review in all the reviews\n",
        "  for review in reviews:   \n",
        "#Here I am extracting rating from page by filtering with the row class    \n",
        "    rating=review.find('div',class_='row').div.text \n",
        "#Now extracting titleReview from page by filtering with the  '_2-N8zT' class     \n",
        "    titleReview=review.find('p',class_='_2-N8zT').text \n",
        "#Now extracting customerName from page by filtering with the '_2sc7ZR _2V5EHH' class     \n",
        "    customerName=review.find('p',class_='_2sc7ZR _2V5EHH').text.strip('[').strip(']')\n",
        "#Here extracting year from page by filtering with the '_2sc7ZR' class     \n",
        "    year=''.join([value.text for index,value in enumerate(review.find_all('p',class_='_2sc7ZR')) if index!=0])\n",
        "#Finally appending all the customerName,titleReview,year and rating to the list     \n",
        "    listValues.append([customerName,titleReview,year,rating]) \n",
        "\n",
        "#creatingNoe I am a data frame using list which has all the details\n",
        "df=pd.DataFrame(listValues,columns=['Customer Name','Review','Year','Rating']) \n",
        "\n",
        "#Using drop function to remove NULL values from the dataframe and inplace=True to make changes in present dataframe\n",
        "df.dropna(inplace=True) \n",
        "df.drop_duplicates(inplace=True) \n",
        "#Now I am droping all the rows that are more than 1000\n",
        "df.drop(df.index[1000:],inplace=True) \n",
        "\n",
        "#Here printing the length of the Dataframe\n",
        "print(\"Length of DataFrame:\",len(df))\n",
        "#Now I am only diplaying 20 records \n",
        "print(df.head(20).to_string(index=False))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu6CWOk8K4Td"
      },
      "source": [
        "Question 3 (10 points). Understand the data quality: Search a second hand dataset (any dataset) from kaggle or other websites. Describe the data quality problem of the dataset and explain your strtegy to clean the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGlfA1krK4Td"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "'''\n",
        "\n",
        "Please write you answer here:\n",
        "Here, I am taking a data set from kaggle called Flight Price Prediction. The main motto is to predic the flight price. Before building the model we need to check the data quality.\n",
        "For this we need to perform Exploratory Data Analysis. In this we can see a lot of data quality problem which need to be rectified before stating with the model.\n",
        "First we need to check the attributes and there types because we cannot use object type attributes and should convert them into int. And check the duplicates and Null values.\n",
        "And moreover converting date format, Date of Journey, Arrival_Time, Source and Destination into required format to perform the desired actions.\n",
        "Below is the strategy for this type of data that I followed:\n",
        "1. Firstly, I imported the required packages.\n",
        "2. Then imported the dataset into train_df.\n",
        "3. Then I checked the components in the data with the help of train_df.head(). Here I can see the columns and there values, by this I can identify the coulumns which are useful and other which I can drop.\n",
        "   Also, what can we do to certain columns without knowing their data types, For example: Date_of_Journey is in DD/MM/YYY so we need to split this into different columns.\n",
        "4. Then I used append function to add test data set to training data set to perform actions on complete data.\n",
        "5. Then by using info() we can get the backend information of complete data. Here I can see most of the columns are in objet data type, which need to changed.\n",
        "6. I took Date_of_Journey to split it into different using str.split('/'). Then store date, Month and Year separetly.\n",
        "7. Then change the data type to int using astype(int)\n",
        "8. Next as the Date_of_Journey is splitted and stored in different columns, we can drop the Date_of_Journey column using drop(Date_of_Journey, axis=1, inplace = True)\n",
        "9. Next I used same process for Arrival_Time to split it and store it in Arrival_Hour and Arrival_min.\n",
        "10. Then change Arrival_Hour and Arrival_min into int datatype. Then droping the Arrival_Time.\n",
        "11. Similarly I have done to Dep_Time.\n",
        "12. Then in Total_Stops column I used map function to code them as 0 stop, 1 stop, 2 stops...\n",
        "13. So then I droped Route column because we already have no.of stops.\n",
        "14. Then we have Duration coulmn, I have repeated same step as Arrival_Time to split the column.\n",
        "15. Then I can find and noise in the data which is showin Duration as 5m for 2 data records, so if I analyse the Source and Destination it from Mumbai to Hyderadad, Which is impossible to complete in 5min. Do we can remove these two rows from the data.\n",
        "16. Then converting the Dur_hour into int and multiplying it into 60 to convert it into min format.\n",
        "17. Then I added Dur_hour, Dur_min and stored it into Trip_Dur\n",
        "18. Next droped both Dur_hour and Dur_min\n",
        "19. Then for the column Airline we have set of unique airlines that are operating. So this cannot be directly used for model building because of its data type. To rectify this I used Label Encoding method.\n",
        "20. By this Label Encoding, the uniques Airlines will be coded with a value.\n",
        "Thus, I have cleaned complete data. After this EDA process, we can go ahead for model building.\n",
        "\n",
        "\n",
        "For your reference, here is code's github link - https://github.com/AmithReddyKasarla/AmithReddy_INFO5502_-Spring2022/blob/main/EDA%20and%20Feature%20Engineering%20on%20Flight%20Price%20Prediction.ipynb\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPdl8-NKK4Te"
      },
      "source": [
        "Question 4 (20 points). Data cleaning: There are two datasets TwADR-L (from Twitter) and AskAPatient (Link: https://zenodo.org/record/55013#.YgU2NN-ZO4T) for medical concept\n",
        "normalization. However, the two datasets have serious data quality problems. Please read the introduction of the datasets and clean the two datasets by following the steps in the statement.\n",
        "\n",
        "In the original dataset, the TwADR-L had 48,057 training, 1,256 validation and 1,427 test examples. The test set (all\n",
        "test samples from 10 folds combined) consists of 765 unique phrases and 273 unique classes (medical concepts). The AskAPatient dataset contained 156,652 training, 7,926 validation, and 8,662 test examples. The entire test set (all test samples\n",
        "from 10 folds combined) consists of 3,749 unique phrases and 1,035 unique classes (medical concepts). The authors\n",
        "randomly split each dataset into ten equal folds, ran 10-fold cross validation and reported the accuracy averaged across the\n",
        "ten folds. \n",
        "\n",
        "We found that, in the original data set, many phrase-label pairs appeared multiple times within the same training data file\n",
        "and also across the training and test data sets in the same fold. In the AskAPatient data set, on average 35.82% of the test data overlapped with training data in the same fold. In the Twitter (TwADR-L) dataset, on average 8.62% of the test set had an overlap with the training data in the same fold. Having a large overlap between the training and the test data can potentially\n",
        "introduce bias in the model and contribute to high accuracy. It is not unlikely that the high model performance reported in the original paper may be triggered by the the large overlap between the training and test sets.\n",
        "\n",
        "Therefore to remove the bias, we further cleaned and recreated the training, validation, and test sets such that each\n",
        "phrase-label pair appears only once in the entire dataset (either in training, validation or test set).\n",
        "\n",
        "(1) First, we combined all examples in training, validation and test data from the original data set and then removed all\n",
        "duplicate phrase-label pairs (examples that have the same phrase and label pair and appear more than once in training/validation/test datasets). Table II shows the statistics of the new dataset (after removing duplicates). The Twitter data set had 3,157 unique phrase-label pairs and 2,220 unique labels (medical concepts) while 173 phrases had multiple labels (i.e., they were assigned to more than one label). Many concepts had only one example, and the concept that had the most number\n",
        "of examples had 36 phrases. On average, each concept had 1.42 examples. The AskAPatient data set had 4,496 unique phrase-label pairs, 1,036 unique labels while 26 phrases had multiple labels. Table III shows examples of phrases that had multiple labels. For example, ‘mad’ can be mapped to ‘anger’ or ‘rage’ and ‘sore’ can be mapped to ‘pain’ or ‘myalgia’.\n",
        "\n",
        "(2) Second, we remove all concepts that had less than five examples. The statistics of the final data are shown in Table IV.\n",
        "\n",
        "(3) Third, we divide all examples without multiple labels into random 10 folds such that each unique phrase-label pair\n",
        "appears once in one of the 10 test sets. We add the pairs with multiple labels into the training data. This final 10-folds\n",
        "dataset is used in all our experiments.\n",
        "\n",
        "(The original paper can be download on canvas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ncgMsprWK4Tf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b80056-b882-4c3a-ec3c-7cf856632945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA STATISTICS AFTER REMOVING DUPLICATES FROM COMBINED TRAINING, VALIDATION, AND TEST DATA\n",
            "                               TwADR-L AskAPatient\n",
            "              # unique phrases    2944        4469\n",
            "               # unique labels    2220        1038\n",
            "   # unique phrase-label pairs    3157        4506\n",
            "# phrases with multiple labels     173          35\n",
            "      Min # examples per label       1           1\n",
            "      Max # examples per label      36         141\n",
            "      Avg # examples per label    1.42        4.34\n",
            "\n",
            "\n",
            "EXAMPLES OF PHRASES WITH MULTIPLE LABELS FOR TwADR-L\n",
            "PHRASE\n",
            "#INSOMNIA                                      INITIAL INSOMNIA,SLEEPLESSNESS\n",
            "A ZOMBIE                                         DEPERSONALIZATION,DROWSINESS\n",
            "ABOUT TO LOSE MY MIND               ABNORMAL MENTAL STATE,DISTURBANCE IN MOOD\n",
            "ADDICTED                                              DRUG ABUSE,DRUG CRAVING\n",
            "ANGRY                                               ANGER,IRRITABLE MOOD,RAGE\n",
            "AWAKE                       SLEEPLESSNESS,ENERGY INCREASED,WAKEFULNESS,SOM...\n",
            "AWAKE APPROX 36-38 HOURS                            WAKEFULNESS,SLEEPLESSNESS\n",
            "BAZAAR THOUGHTS                          HALLUCINATIONS,ABNORMAL MENTAL STATE\n",
            "BIPOLAR                     MOOD SWINGS,BIPOLAR DISORDER,PSYCHOTIC DEPRESSION\n",
            "BRAIN FUG                        ABNORMAL MENTAL STATE,CONSCIOUSNESS ABNORMAL\n",
            "Name: LABEL, dtype: object\n",
            "\n",
            "\n",
            "EXAMPLES OF PHRASES WITH MULTIPLE LABELS FOR AskAPatient\n",
            "PHRASE\n",
            "BACK PAIN                                            SEVERE PAIN,BACKACHE\n",
            "EXTREME STOMACH PAIN                           ABDOMINAL PAIN,SEVERE PAIN\n",
            "GENERAL MALAISE              EXCESSIVE UPPER GASTROINTESTINAL GAS,MALAISE\n",
            "HIGH BLOOD PRESSURE     INCREASED VENOUS PRESSURE,FINDING OF INCREASED...\n",
            "HIGH BP                 INCREASED VENOUS PRESSURE,FINDING OF INCREASED...\n",
            "LEG CRAMPS                         CRAMP IN LOWER LIMB,CRAMP IN LOWER LEG\n",
            "LOWER ABDOMINAL PAIN                  ABDOMINAL PAIN,LOWER ABDOMINAL PAIN\n",
            "MUSCLE ACHES IN ARMS    MYALGIA/MYOSITIS - FOREARM,MYALGIA,MYALGIA/MYO...\n",
            "MUSCLE FATIGUE                                     MUSCLE FATIGUE,MYALGIA\n",
            "NUMBNESS IN TOES                         NUMBNESS OF FOOT,NUMBNESS OF TOE\n",
            "Name: LABEL, dtype: object\n",
            "\n",
            "\n",
            "DATA STATISTICS AFTER REMOVING CONCEPTS THAT HAVE LESS THAN FIVE EXAMPLES\n",
            "                               TwADR-L AskAPatient\n",
            "              # unique phrases     616        2664\n",
            "               # unique labels      76         233\n",
            "   # unique phrase-label pairs     721        2685\n",
            "# phrases with multiple labels      87          19\n",
            "      Min # examples per label       5           5\n",
            "      Max # examples per label      36         141\n",
            "      Avg # examples per label    9.49       11.52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/groupby/generic.py:303: FutureWarning: Dropping invalid columns in SeriesGroupBy.agg is deprecated. In a future version, a TypeError will be raised. Before calling .agg, select only columns which should be valid for the aggregating function.\n",
            "  results[key] = self.aggregate(func)\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "#importing pandas to perform the data analysis\n",
        "import pandas as pd\n",
        "\n",
        "#Creating a Method to read the test,train and validation files from github datasets\n",
        "def generateDataFrame(inputDataSet:str,inputFile:str)->pd.DataFrame:\n",
        "  df=pd.DataFrame(); \n",
        "#Here I am taking range 10 to read 10 files for each typei the dataset - test,train and validation\n",
        "  for index in range(10): \n",
        "    url='https://raw.githubusercontent.com/AmithReddyKasarla/AmithReddy_INFO5502_-Spring2022/main/'+inputDataSet+'/'+inputFile.capitalize()+'/'+inputDataSet+'.fold-'+str(index)+'.'+inputFile.lower()+'.txt'\n",
        "    temp=pd.read_csv(url,sep='\\t',encoding = 'unicode_escape',header=None) #reading from text file\n",
        "    df=df.append(temp)\n",
        "  return df\n",
        "\n",
        "#Method to write the test,train files for both the datasets\n",
        "def createFinalFolds(df,dataSet,file):\n",
        "    noOfRows=int(len(df)/10)\n",
        "#Here I am considering a range of 10 to generate 10 folds\n",
        "    for i in range(10):\n",
        "#Now when the file is last one then we consider to write all the remaining rows into the file       \n",
        "      if i==9: \n",
        "        temp=df\n",
        "      else:\n",
        "#Here I am randomly selecting 10% of rows from the dataframe to store in each fold        \n",
        "        temp=df.sample(n = noOfRows) \n",
        "\n",
        "      temp[['ID','LABEL','PHRASE']].to_csv(''+dataSet+'.fold-'+str(i)+'.'+file+'.txt',sep='\\t',index=False,header=False) #writing to text file\n",
        "      df.drop(temp.index,inplace=True)\n",
        "\n",
        "def output():\n",
        "  TwADRLDataSet=pd.DataFrame();\n",
        "  askAPatientDataSet=pd.DataFrame();\n",
        "\n",
        "  Table2List=list()\n",
        "  Table4List=list()\n",
        "\n",
        "  files=['Test','Train','Validation'] #3 types of file types\n",
        "  dataSets=['TwADR-L','AskAPatient'] #2 types of datasets\n",
        "#Now I am looping over the datasets\n",
        "  for dataset in dataSets:\n",
        "#Next looping over file types     \n",
        "    for file in files: \n",
        "      if dataset=='TwADR-L':\n",
        "          TwADRLDataSet=TwADRLDataSet.append(generateDataFrame(dataset,file),ignore_index=True) #storing all the contents of 'TwADR-L' dataset into the dataframe(TwADRLDataSet)\n",
        "      elif dataset=='AskAPatient':\n",
        "        askAPatientDataSet=askAPatientDataSet.append(generateDataFrame(dataset,file),ignore_index=True) #storing all the contents of 'AskAPatient' dataset into the dataframe(askAPatientDataSet)\n",
        "\n",
        "#Assigning the given column names\n",
        "  headers=['ID','LABEL','PHRASE']\n",
        "\n",
        "# Now i will give the header names and strip/upper the values for the dataframe 'TwADRLDataSet'\n",
        "  TwADRLDataSet.columns=headers\n",
        "  TwADRLDataSet['LABEL']=TwADRLDataSet['LABEL'].str.strip(' ').str.upper()\n",
        "  TwADRLDataSet['PHRASE']=TwADRLDataSet['PHRASE'].str.strip(' ').str.upper()\n",
        "\n",
        "#Here giving header names and strip/upper the values for the dataframe 'askAPatientDataSet'\n",
        "  askAPatientDataSet.columns=headers\n",
        "  askAPatientDataSet['LABEL']=askAPatientDataSet['LABEL'].str.strip(' ').str.upper()\n",
        "  askAPatientDataSet['PHRASE']=askAPatientDataSet['PHRASE'].str.strip(' ').str.upper()\n",
        "\n",
        "#Now removing duplicate rows/NULL values from both the data frames\n",
        "  askAPatientDataSet.drop_duplicates(subset=['LABEL','PHRASE'],keep=\"first\",inplace=True)\n",
        "  askAPatientDataSet.dropna(inplace=True)\n",
        "  TwADRLDataSet.drop_duplicates(subset=['LABEL','PHRASE'],keep=\"first\",inplace=True)\n",
        "  TwADRLDataSet.dropna(inplace=True)\n",
        "  \n",
        "#Finally Obtaining result for Table II\n",
        "  Table2List.append(['# unique phrases',len(TwADRLDataSet['PHRASE'].unique()),len(askAPatientDataSet['PHRASE'].unique())])\n",
        "  Table2List.append(['# unique labels',len(TwADRLDataSet['LABEL'].unique()),len(askAPatientDataSet['LABEL'].unique())])\n",
        "  Table2List.append(['# unique phrase-label pairs',len(TwADRLDataSet),len(askAPatientDataSet)])\n",
        "  Table2List.append(['# phrases with multiple labels',len(pd.DataFrame(TwADRLDataSet.groupby(['PHRASE']).LABEL.count()).loc[pd.DataFrame(TwADRLDataSet.groupby(['PHRASE']).LABEL.count())['LABEL']>1]),len(pd.DataFrame(askAPatientDataSet.groupby(['PHRASE']).LABEL.count()).loc[pd.DataFrame(askAPatientDataSet.groupby(['PHRASE']).LABEL.count())['LABEL']>1])])\n",
        "  Table2List.append(['Min # examples per label',pd.DataFrame(TwADRLDataSet.groupby(['LABEL']).PHRASE.count())['PHRASE'].min(),pd.DataFrame(askAPatientDataSet.groupby(['LABEL']).PHRASE.count())['PHRASE'].min()])\n",
        "  Table2List.append(['Max # examples per label',pd.DataFrame(TwADRLDataSet.groupby(['LABEL']).PHRASE.count())['PHRASE'].max(),pd.DataFrame(askAPatientDataSet.groupby(['LABEL']).PHRASE.count())['PHRASE'].max()])\n",
        "  Table2List.append(['Avg # examples per label',str(round(pd.DataFrame(TwADRLDataSet.groupby(['LABEL']).PHRASE.count())['PHRASE'].mean(),2)),str(round(pd.DataFrame(askAPatientDataSet.groupby(['LABEL']).PHRASE.count())['PHRASE'].mean(),2))])\n",
        "  Table2DataSet=pd.DataFrame(Table2List,columns=['','TwADR-L','AskAPatient'])\n",
        "\n",
        "  print('DATA STATISTICS AFTER REMOVING DUPLICATES FROM COMBINED TRAINING, VALIDATION, AND TEST DATA')\n",
        "  print(Table2DataSet.to_string(index=False))\n",
        "  \n",
        "#Now Calculating and storing the count of examples associated to each medical concept for both the datasets\n",
        "  TwADRLDataSet['PHRASE COUNTS']=TwADRLDataSet.groupby(['LABEL'])['PHRASE'].transform('count')\n",
        "  askAPatientDataSet['PHRASE COUNTS']=askAPatientDataSet.groupby(['LABEL'])['PHRASE'].transform('count')\n",
        "\n",
        "#Here I am Deleting the medical concepts that had less than five examples\n",
        "  TwADRLDataSet.drop(TwADRLDataSet.loc[TwADRLDataSet['PHRASE COUNTS']<5].index,inplace=True)\n",
        "  askAPatientDataSet.drop(askAPatientDataSet.loc[askAPatientDataSet['PHRASE COUNTS']<5].index,inplace=True)\n",
        "\n",
        "#Calculating and storing the count of medical concepts associated to each phrase for both the datasets\n",
        "  TwADRLDataSet['LABEL COUNTS']=TwADRLDataSet.groupby(['PHRASE'])['LABEL'].transform('count')\n",
        "  askAPatientDataSet['LABEL COUNTS']=askAPatientDataSet.groupby(['PHRASE'])['LABEL'].transform('count')\n",
        "\n",
        "#storing the Phrase that has no multiple labels in Test Dataset \n",
        "  TwADRLTestDataset=TwADRLDataSet.loc[TwADRLDataSet['LABEL COUNTS']==1].copy()\n",
        "  askAPatientTestDataset=askAPatientDataSet.loc[askAPatientDataSet['LABEL COUNTS']==1].copy()\n",
        "\n",
        "#storing the Phrase that has multiple labels in Training Dataset \n",
        "  TwADRLTrainingDataset=TwADRLDataSet.loc[TwADRLDataSet['LABEL COUNTS']>1].copy()\n",
        "  askAPatientTrainingDataset=askAPatientDataSet.loc[askAPatientDataSet['LABEL COUNTS']>1].copy()\n",
        "\n",
        "#Now, Obtaining result for Table III\n",
        "  print(\"\\n\")\n",
        "  print('EXAMPLES OF PHRASES WITH MULTIPLE LABELS FOR TwADR-L')\n",
        "  print(TwADRLTrainingDataset.groupby('PHRASE').agg(lambda x : ','.join(x))['LABEL'].head(10))\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print('EXAMPLES OF PHRASES WITH MULTIPLE LABELS FOR AskAPatient')\n",
        "  print(askAPatientTrainingDataset.groupby('PHRASE').agg(lambda x : ','.join(x))['LABEL'].head(10))\n",
        "\n",
        "#Obtaining result for Table IV\n",
        "  Table4List.append(['# unique phrases',len(TwADRLDataSet['PHRASE'].unique()),len(askAPatientDataSet['PHRASE'].unique())])\n",
        "  Table4List.append(['# unique labels',len(TwADRLDataSet['LABEL'].unique()),len(askAPatientDataSet['LABEL'].unique())])\n",
        "  Table4List.append(['# unique phrase-label pairs',len(TwADRLDataSet),len(askAPatientDataSet)])\n",
        "  Table4List.append(['# phrases with multiple labels',len(pd.DataFrame(TwADRLDataSet.groupby(['PHRASE']).LABEL.count()).loc[pd.DataFrame(TwADRLDataSet.groupby(['PHRASE']).LABEL.count())['LABEL']>1]),len(pd.DataFrame(askAPatientDataSet.groupby(['PHRASE']).LABEL.count()).loc[pd.DataFrame(askAPatientDataSet.groupby(['PHRASE']).LABEL.count())['LABEL']>1])])\n",
        "  Table4List.append(['Min # examples per label',pd.DataFrame(TwADRLDataSet.groupby(['LABEL']).PHRASE.count())['PHRASE'].min(),pd.DataFrame(askAPatientDataSet.groupby(['LABEL']).PHRASE.count())['PHRASE'].min()])\n",
        "  Table4List.append(['Max # examples per label',pd.DataFrame(TwADRLDataSet.groupby(['LABEL']).PHRASE.count())['PHRASE'].max(),pd.DataFrame(askAPatientDataSet.groupby(['LABEL']).PHRASE.count())['PHRASE'].max()])\n",
        "  Table4List.append(['Avg # examples per label',str(round(pd.DataFrame(TwADRLDataSet.groupby(['LABEL']).PHRASE.count())['PHRASE'].mean(),2)),str(round(pd.DataFrame(askAPatientDataSet.groupby(['LABEL']).PHRASE.count())['PHRASE'].mean(),2))])\n",
        "  Table4DataSet=pd.DataFrame(Table4List,columns=['','TwADR-L','AskAPatient'])\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print('DATA STATISTICS AFTER REMOVING CONCEPTS THAT HAVE LESS THAN FIVE EXAMPLES')\n",
        "  print(Table4DataSet.to_string(index=False))\n",
        "  \n",
        "#writing the final data to 10 text folds\n",
        "  createFinalFolds(TwADRLTestDataset,'TwADR-L','test')\n",
        "  createFinalFolds(TwADRLTrainingDataset,'TwADR-L','train')\n",
        "  createFinalFolds(askAPatientTestDataset,'AskAPatient','test')\n",
        "  createFinalFolds(askAPatientTrainingDataset,'AskAPatient','train')\n",
        "  \n",
        "output()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "lab_assignment_03.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}